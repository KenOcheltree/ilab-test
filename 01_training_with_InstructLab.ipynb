{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training with InstructLab\n",
    "\n",
    "<ul>\n",
    "<li>Contributors: InstructLab team and IBM Research Technology Education team\n",
    "<li>Contact for questions and technical support: IBM.Research.JupyterLab@ibm.com\n",
    "<li>Provenance: IBM Research\n",
    "<li>Version: 1.0.8\n",
    "<li>Release date: 2024-11-14\n",
    "<li>Compute requirements: GPU: estimated 30 minutes (19 min for cell 12 and 10 min for cell 15 )\n",
    "<li>Memory requirements: 16 GB\n",
    "<li>Notebook set: InstructLab\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select Viewing Option \n",
    "\n",
    "This notebook was optimized for viewing the output in a separate panel:\n",
    "- If you would like to see the separate panel output, set `dual_screen` to *True* in the first line of the next cell and follow the steps below. \n",
    "- If you want the output inline with the notebook cells, set `dual_screen` to *False*. If you are running with the output inline with the notebook, please run the notebook cell by cell so that options can be selected.\n",
    "\n",
    "If you set `dual_screen` to *True*, perform the following:\n",
    "1. Right click on the same cell and select **Create New View for Output**.\n",
    "1. Drag the new **Output View** panel to the right side of the JupyterLab.\n",
    "1. Hide the File Browser by toggling the File Browser icon on the top left of the JupyterLab.\n",
    "1. To run the notebook, click on a notebook code cell, then from the top menu select *Kernel->Restart Kernel and Run All Cells*.\n",
    "1. Select options and the *Continue* button to progress with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dual_screen = False\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import ipywidgets as widgets\n",
    "from ipynb_pause import flow\n",
    "\n",
    "H1 = \"<p style='font-family:IBM Plex Sans;font-size:28px'>\"\n",
    "H2 = \"<p style='font-family:IBM Plex Sans;font-size:24px'>\"\n",
    "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
    "Small = \"<p style='font-family:IBM Plex Sans;font-size:17px'>\"\n",
    "Ex = \"<p style='font-family:IBM Plex Sans;font-size:20px;font-style:italic'>\"\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "run=flow.display_mode(mode=dual_screen, output=out, color='darkblue')\n",
    "if dual_screen:\n",
    "    display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "This notebook demonstrates InstructLab, a model-agnostic open source AI project that facilitates contributions to Large Language Models (LLMs).\n",
    "\n",
    "This notebook is part of a sequential notebook set. Before using this notebook, please ensure that you have run the first notebook in this  set: [Configuring InstructLab](./00_configuring_InstructLab.ipynb).\n",
    "\n",
    "In this notebook, we will demonstrate the following:\n",
    "- Querying the LLM before \n",
    "- Creating a question and answer data file\n",
    "- Generating synthetic data for training\n",
    "- Training the LLM with the generated data\n",
    "\n",
    "**Notes:** This notebook must be run with a GPU. If you are not running with a GPU, please select File->Hub Control Panel->Stop My Server, then Start My Server and the select GPU Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Step 0. Introduction](#I2_intro)\n",
    "* [Step 1. Import Libraries and Check Configuration](#I2_import)\n",
    "* [Step 2. Specify the Data for this Run](#I2_data)\n",
    "* [Step 3. Create the Taxonomy Data Repository](#I2_taxonomy)\n",
    "* [Step 4. Generate Synthetic Data](#I2_generate)\n",
    "* [Step 5. Train Model](#I2_train)\n",
    "* [Conclusion](#I2_conclusion)\n",
    "* [Learn more](#I2_learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"#I2_intro\"></a>\n",
    "# Step 0. Introduction\n",
    "\n",
    "This notebook provides a template for running InstructLab in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 0. Introduction\"))\n",
    "        display(Image(filename='data/images/Flow.png',width=1000))\n",
    "else: \n",
    "    display(Image(filename='data/images/Flow.png',width=1200))\n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"I2_import\"></a>\n",
    "# Step 1. Import Libraries and Check Configuration\n",
    "\n",
    "## 1.1. Imports and configuration\n",
    "\n",
    "This code cell also checks for GPU availability. This notebook requires a GPU to run in a reasonable time.\n",
    "\n",
    "Check that InstructLab version 0.23.1 is installed properly and is configured for using a GPU.\n",
    "\n",
    "The first line from 'InstructLab' section should read\n",
    "```\n",
    "instructlab.version: 0.23.1\n",
    "```\n",
    "and the last line should read\n",
    "```\n",
    "llama_cpp_python.supports_gpu_offload: True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import json\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
    "il_data_path= '/home/jovyan/.local/share/instructlab/datasets/'\n",
    "with open('config.json', 'r') as f:\n",
    "    jsonData = json.load(f)\n",
    "with open('instructlab.json', 'r') as f:\n",
    "    jsonState = json.load(f)    \n",
    "\n",
    "# torch and cuda version check\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 1. Import Libraries and Check Configuration\"))\n",
    "        display(widgets.HTML(H1+\"1.1 Imports and configuration\"))\n",
    "        display(widgets.HTML(Norm+\"Perform imports and see if a GPU is available\"))\n",
    "        print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "        if torch.cuda.is_available() is False:\n",
    "            display(widgets.HTML(Norm+\"ERROR: GPU not in configuration, Please restart with a GPU\"))\n",
    "            run.resume() \n",
    "        else:\n",
    "            os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "            print(\"GPU is Available\\n\")\n",
    "            display(widgets.HTML(Norm+\"Check if configuration supports GPU offload\"))\n",
    "            !ilab system info\n",
    "        \n",
    "else: \n",
    "    print(f\"Imports completed\")\n",
    "    print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "    if torch.cuda.is_available() is False:\n",
    "        print(\"ERROR: GPU not in configuration, Please restart with a GPU\")\n",
    "    else:\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "        print(\"GPU is Available\\n\")\n",
    "        !ilab system info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Optionally test the base model before adding data\n",
    "\n",
    "At this point you may wish to run an InstructLab server and run queries against the base model. \n",
    "\n",
    "This may be useful if you are working with new content and want to query the base model to ascertain the responses before InstructLab training.\n",
    "\n",
    "After training, the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook allows you to ask questions to both the base and InstructLab trained models and compare answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H1+\"1.2 Optionally test the base model before adding data\"))\n",
    "    \n",
    "#!ilab model serve --model-path models/merlinite-7b-lab-Q4_K_M.gguf    \n",
    "    \n",
    "run.pause()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I2_data\"></a>\n",
    "# Step 2. Specify the Data for this Run\n",
    "\n",
    "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\" and \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Optionally, Create your own data set for InstructLab\n",
    "\n",
    "Follow these steps to add your own dataset:\n",
    "1. Create your own **qna.yaml** file based on the example qna.yaml files provided in the /data/oscars, /data/quantum and /data/agentic_ai directories. Additional guidance on creating a properly formatted QNA.yaml file is found on the [InstructLab taxonomy readme](https://github.com/instructlab/taxonomy).\n",
    "1. Add your **qna.yaml** and sample **questions.txt** files to the **/data/your_content_1** folder or the **/data/your_content_2** folder.\n",
    "1. Right click on the **config.json** file and select Open With->Editort. Specify the **qna_location** where your data resides within the Dewy Decimal classification system. Close and save the **config.json** file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = widgets.ToggleButtons(\n",
    "    options=['2024 Oscars', 'Quantum', 'Agentic AI', 'Your Content 1', 'Your Content 2'],\n",
    "    tooltips=['2024 Oscar Awards Ceremony', 'Quantum Roadmap and Patterns', 'Artificial Intelligence Agents', 'Your own uploaded content dataset 1', 'Your own uploaded content dataset 2'],\n",
    "    description='Dataset:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "print(\"\\nSelect the QNA dataset to add:\")\n",
    "display(data_set)\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 2. Choose the Dataset for this Run\"))\n",
    "        display(widgets.HTML(Norm+\"<br>Select content for InstructLab processing from the following:\"))\n",
    "        display(data_set)\n",
    "        display(widgets.HTML(Small+\"Note: Choose <b>Demo Content</b> or <b>Your Own Content</b> if you are providing your own created QNA file\"))\n",
    "else:\n",
    "    print(\"After choosing your dataset for this run, please select and run the following cell\")        \n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H1+\"Step 2. Choose the Dataset for this Run\"))\n",
    "        \n",
    "if data_set.value=='2024 Oscars':\n",
    "    use_case=\"oscars\"\n",
    "elif data_set.value=='Quantum':\n",
    "    use_case=\"quantum\"\n",
    "elif data_set.value=='Agentic AI':\n",
    "    use_case=\"agentic_ai\"\n",
    "elif data_set.value=='Your Content 1':\n",
    "    use_case=\"your_content_1\"\n",
    "elif data_set.value=='Your Content 2':\n",
    "    use_case=\"your_content_2\"\n",
    "else:\n",
    "    use_case=\"undefined\"\n",
    "    \n",
    "if use_case==\"undefined\":   \n",
    "    print(\"ERROR: Undefined data set: \" + data_set.value + \" data\")\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"ERROR: Undefined data set: \" + data_set.value + \" data\"))\n",
    "    run.pause()\n",
    "else:\n",
    "    jsonState[\"last_use_case\"]=data_set.value\n",
    "    with open('instructlab.json', 'w') as f:\n",
    "            json.dump(jsonState, f, indent=4)\n",
    "    qna_file=\"data/\" + use_case + \"/qna.yaml\"\n",
    "    qna_location=jsonData[\"use_cases\"][use_case][\"qna_location\"]            \n",
    "\n",
    "    print(\"Using \" + data_set.value + \" data\")\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"Using \" + data_set.value + \" data\"))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I2_taxonomy\"></a>\n",
    "# Step 3. Create the Taxonomy Data Repository\n",
    "## 3.1 Delete the prior repository and clone the empty taxonomy repository\n",
    "We start with an empty repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_command1 = f\"rm -rf taxonomy\"\n",
    "taxonomy_repo=jsonData[\"taxonomy_repo\"]\n",
    "shell_command2 = f\"git clone {taxonomy_repo}\"\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 3. Create the Taxonomy Data Repository\"))\n",
    "        display(widgets.HTML(H2+\"3.1 Delete the prior repository and clone the empty taxonomy repository\"))\n",
    "        !{shell_command1}\n",
    "        !{shell_command2}\n",
    "else:\n",
    "    print(\"Step 3. Create the Taxonomy data Repository\")\n",
    "    print(\"3.1 Delete the prior repository and clone the empty taxonomy repository\")\n",
    "    !{shell_command1}\n",
    "    !{shell_command2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2 View the beginning of the QNA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file_top():\n",
    "    print_lines=40\n",
    "    with open(qna_file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            if line_number > print_lines:  # line_number starts at 0.\n",
    "                break\n",
    "            print(line, end=\"\")           \n",
    "\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H2+\"3.2 View the beginning of the QNA file\"))\n",
    "        print_file_top()\n",
    "else:\n",
    "    print(\"3.2 Show QNA file\")\n",
    "    print_file_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3 Place the QNA file in the proper taxonomy directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should produce !mkdir -p ./taxonomy/knowledge/textbooks/culture/movies/awards/oscars\n",
    "shell_command1 = f\"mkdir -p ./taxonomy/{qna_location}\"\n",
    "shell_command2 = f\"cp ./{qna_file} ./taxonomy/{qna_location}/qna.yaml\"\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H2+\"3.3 Place the QNA file in the proper taxonomy directory\"))\n",
    "        display(widgets.HTML(Norm+\"Place QNA file in taxonomy as: /taxonomy/\"+qna_location+\"/qna.yaml\"))\n",
    "        !{shell_command1}\n",
    "        !{shell_command2}\n",
    "else:\n",
    "    print(\"3.3 Place the QNA file in the proper taxonomy directory\")\n",
    "    print(\"Place QNA file in taxononmy as: /taxonomy/\"+qna_location+\"/qna.yaml\")\n",
    "    !{shell_command1}\n",
    "    !{shell_command2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Verify the taxonomy\n",
    "We run the ilab taxonomy diff command to verify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H2+\"3.4 Verify the taxonomy\"))\n",
    "        !ilab taxonomy diff\n",
    "else:\n",
    "    print(\"Verify the taxonomy\")\n",
    "    !ilab taxonomy diff\n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"I2_generate\"></a>\n",
    "# Step 4. Generate Synthetic Data\n",
    "\n",
    "This step will produce synthetic training data from the provided repository in the form of question and answer pairs. To generate synthetic data, InstructLab uses s large teacher model, such as Mixtral 8x7B, to create synthetic training data about the manually created data to train a small student model, such as the Merlinite 7B or Granite 7B models.\n",
    "\n",
    "You can skip this step and use the previously generated 500 synthetic samples found in the *generated* folder.\n",
    "\n",
    "## 4.1 Set data generation parameters\n",
    "\n",
    "### Select pipeline\n",
    "\n",
    "InstructLab has three primary pipelines that can be used: simple, full and acellerated:\n",
    "- The **simple pipeline** runs fast and can be used for initial model and data testing. \n",
    "- The **full pipeline** runs all of the InstrctLab steps and takes more time but produces a better tuned model. \n",
    "- The **accelerated pipeline** runs the full pipeline processing using a GPU, so it produces a similarly tuned model in a shorter time.\n",
    "\n",
    "**Note:** If you are running with a new or modifed dataset, you may want to use the **Simple pipeline** for the first run to verify the configuration\n",
    "\n",
    "### Sepect number of samples to generate\n",
    "\n",
    "Data generation takes 19 minutes for generating 15 synthetic data samples. You may wish to generate a small number on your first run to verify the QNA dataset format.\n",
    "\n",
    "To produce **sufficient synthetic data** to focus training on the new material, **about 30 synthetic questions and answer pairs need to be generated** for each question and answer pair provided. This will require a proportionally longer time to generate, but will provide better training.\n",
    "\n",
    "Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, ilab data generate can start a server for you if you provide a fully qualified model path via --model.\n",
    "\n",
    "To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n",
    "\n",
    "    ilab data generate\n",
    "\n",
    "### **Simple Pipeline**\n",
    "\n",
    "The Simple Pipeline works solely with Merlinite 7b Lab as the teacher model. The Simple Pipeline is called without GPU acceleration as follows:\n",
    "\n",
    "    ilab data generate --pipeline simple\n",
    "\n",
    "### **Full Pipeline**\n",
    "\n",
    "The Full Pipeline runs the full processing with a GPU. Currently, the Full Pipeline only supports the Mixtral and Mistral Instruct Family models as the teacher model.  This is due to only supporting specific model prompt templates.\n",
    "\n",
    "Using a non-default model such as Mixtral-8x7B-Instruct-v0.1) to generate data with the Full Pipeline:\n",
    "\n",
    "    ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n",
    "\n",
    "**Note** Synthetic Data Generation can take from 15 minutes to 1+ hours to complete, depending on your computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe2 = widgets.ToggleButtons(\n",
    "    options=['Simple', 'Full with GPU', 'Demo (Use prior data)'],\n",
    "    tooltips=['Ilab Simple Pipeline', 'Full Pipe running on a GPU', 'Demo Run with previously created data'],\n",
    "    description='Processing:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "instr=widgets.ToggleButtons(\n",
    "    options=['15', '50', '200', '450 (default)', '1000'],\n",
    "    description='# of QNAs:',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    style={\"button_width\": \"auto\"}\n",
    ")\n",
    "\n",
    "print(\"Select Pipeline to use\")\n",
    "display(pipe2)\n",
    "display(instr)\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 4. Generate Synthetic Data\"))\n",
    "        display(pipe2)\n",
    "        display(instr)\n",
    "else:\n",
    "    print(\"After making your selections for data generation, please select and run the following cell\")        \n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.2 Run data generation\n",
    "Data generation takes 19 minutes for generating 15 synthetic data samples and takes longer to generate more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
    "if pipe2.value!='Demo (Use prior data)':\n",
    "    if instr.value == '15':\n",
    "        sdg_factor=\"--sdg-scale-factor 1\"\n",
    "    elif instr.value == '50':\n",
    "        sdg_factor=\"--sdg-scale-factor 3\"\n",
    "    elif instr.value == '200':\n",
    "        sdg_factor=\"--sdg-scale-factor 13\"\n",
    "    elif instr.value == '450 (default)':\n",
    "        sdg_factor=\"\"\n",
    "    else:\n",
    "        sdg_factor=\"--sdg-scale-factor 67\"\n",
    "    # 'Fast (Simple)', 'Full with CPU'\n",
    "    if pipe2.value == 'Simple':\n",
    "        pipeline = 'simple'\n",
    "        model = 'models/merlinite-7b-lab-Q4_K_M.gguf'\n",
    "        gpus = '--gpus 1'\n",
    "    elif pipe2.value == 'Full with GPU':\n",
    "        pipeline = 'full'\n",
    "        model = 'models/mistral-7b-instruct-v0.2.Q4_K_M.gguf'\n",
    "        gpus = '--gpus 1'\n",
    "    else:\n",
    "        if dual_screen:\n",
    "            with out:\n",
    "                display(widgets.HTML(Norm+\"ERROR: Undefined pipeline\"))\n",
    "    \n",
    "    #Remove old data so there is only one test_merlinite and train_merlinite after generation\n",
    "    !rm -rf /home/jovyan/.local/share/instructlab/datasets/*\n",
    "    #shell_command = f\"ilab --verbose data generate --model {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
    "    shell_command = f\"ilab data generate --model {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(H2+\"4.2 Run data generation (over 19 min)\"))\n",
    "            display(widgets.HTML(Norm+\"Running:<br> !\"+shell_command))\n",
    "            !{shell_command}\n",
    "    else:\n",
    "        print(\"Generating data\")\n",
    "        print(\"Running: !\"+shell_command)\n",
    "        !{shell_command}\n",
    "\n",
    "    #Rename results to  test_gen.jsonl and train_gen.jsonl and move to local data directory\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"Create directory: \" + directory)\n",
    "        !mkdir {directory}\n",
    "    file_cnt=0\n",
    "    for dirname in os.listdir(il_data_path):\n",
    "        date_path=il_data_path+'/'+ dirname + '/'\n",
    "        for filename in os.listdir(date_path):\n",
    "            if filename[:6]=='train_':\n",
    "                train_name= 'train_gen.jsonl'\n",
    "                print('Renaming '+ filename+ ' to ' + train_name)\n",
    "                !mv {date_path+filename} {directory+train_name}\n",
    "                file_cnt+=1\n",
    "            elif filename[:5]=='test_':\n",
    "                test_name= 'test_gen.jsonl'\n",
    "                print('Renaming '+ filename+ ' to ' + test_name)\n",
    "                !mv {date_path+filename} {directory+test_name}\n",
    "                file_cnt+=1\n",
    "    if file_cnt < 2:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"ERROR: train_gen.jsonl and/or test.jsonl not created\"))\n",
    "        print(\"ERROR: train_gen.jsonl and/or test.jsonl not created\") \n",
    "    elif os.path.getsize(directory+train_name) == 0:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"ERROR: train_gen.jsonl file is empty\"))\n",
    "        print(\"ERROR: train_gen.jsonl file is empty\")\n",
    "    elif os.path.getsize(directory+test_name) == 0:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"ERROR: test_gen.jsonl file is empty\"))\n",
    "        print(\"ERROR: test_gen.jsonl file is empty\")\n",
    "    else:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"Training and test files successfully created in: \" + directory))\n",
    "        print(\"Training and test files successfully created in: \" + directory)\n",
    "        \n",
    "else:    \n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"Using previously generated data\"))\n",
    "    print(\"Using previously generated data\")\n",
    "run.pause()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Show examples of generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H2+\"4.3 Show examples of generated data\"))\n",
    "else:\n",
    "    print(\"4.3 Show examples of generated data\")\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename[:9]=='train_gen':\n",
    "        with open(directory+filename, 'r') as syn_file:\n",
    "            cnt=0\n",
    "            for line_number, line in enumerate(syn_file):\n",
    "                if cnt >= 8:\n",
    "                    break\n",
    "                jsonLine= json.loads(line)\n",
    "                syn_user=jsonLine[\"user\"]\n",
    "                syn_assist=jsonLine[\"assistant\"]\n",
    "                #Remove \"Answer:\" and \"Response:\" from anawers for displaying\n",
    "                if syn_assist[:8]==\"Answer: \":\n",
    "                    syn_assist=syn_assist[8:]\n",
    "                cnt+=1\n",
    "                if dual_screen:\n",
    "                    with out:\n",
    "                        display(widgets.HTML(Norm+\"Question: \"+syn_user + \"<br>Answer: \" + syn_assist))\n",
    "                else:\n",
    "                    print(\"\\nQuestion: \"+syn_user+\"\\nAnswer: \"+syn_assist)\n",
    "                                    \n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvTYe8oHdm13",
    "tags": []
   },
   "source": [
    "<a id=\"I2_train\"></a>\n",
    "# Step 5. Train the Model\n",
    "\n",
    "## 5.1 Select the model training pipeline\n",
    "\n",
    "InstructLab has three primary model training pipelines: simple, full (default), and accelerated. For all of the models, the training time can be limited by adjusting the num_epoch paramater. The maximum number of epochs for running the InstructLab end-to-end workflow is 10.\n",
    "\n",
    "### **Simple pipeline**\n",
    "\n",
    "The simple pipeline uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process. The simple pipeline only works with Merlinite 7b Lab as the teacher model. For this Linux system, the trained model is saved in the models directory as ggml-model-f16.gguf.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train --pipeline simple\n",
    "\n",
    "**Note:** This process will take a little while to complete (time can vary based on hardware and output of ilab data generate but on the order of 5 to 15 minutes)\n",
    "\n",
    "### **Full pipeline**\n",
    "\n",
    "The full pipeline uses a custom training loop and data processing functions for the granite family of models. This loop is optimized for CPU and MPS functionality. Please use **--pipeline=full** in combination with **--device=cpu** for this Linus system. For a MacOS system you can use --device=mps (MacOS) or --device=cpu, however, MPS is optimized for better performance on MacOS systems. The full pipeline only works with Mixtral and Mistral Instruct Family models as the teacher model. For the full pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train\n",
    "\n",
    "\n",
    "**Note:** This process will take a while to complete. If you run for ~8 epochs it will take several hours.\n",
    "\n",
    "### **Accelerated pipeline**\n",
    "\n",
    "The accelerated uses the instructlab-training library which supports GPU accelerated and distributed training. The full loop and data processing functions are either pulled directly from or based off of the work in this library. For the accelerated pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one. Training is support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>\n",
    "\n",
    "### **Multiphase**\n",
    "\n",
    "When running multi phase training evaluation is run on each phase, we will tell you which checkpoint in this folder performs the best. Train the model locally with multi-phase training and GPU acceleration. This results in the following workflow:\n",
    "1. We train the model on knowledge\n",
    "1. Evaluate the trained model to find the best checkpoint\n",
    "1. We train the model on skills\n",
    "1. We evaluate the model to find the best overall checkpoint\n",
    "\n",
    "Phase 1 models saved in ~/.local/share/instructlab/phased/phase1/checkpoints (Knowledge training). Phase 2 models saved in ~/.local/share/instructlab/phased/phase2/checkpoints (Skills training). Evaluation is run for phase 2 to identify the best checkpoint.\n",
    "\n",
    "A multiphase training is of the form:\n",
    "\n",
    "    ilab model train --strategy lab-multiphase --phased-phase1-data <knowledge train messages jsonl> --phased-phase2-data <skills train messages jsonl> -y\n",
    "\n",
    "**Note:** This command may take 3 or more hours depending on the size of the data and number of training epochs you run.\n",
    "\n",
    "### **Skills only**\n",
    "\n",
    "Phase 2 models saved in ~/.local/share/instructlab/phased/phase2/checkpoints (Skills training). Evaluation is run for phase 2 to identify the best checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pipe3 = widgets.ToggleButtons(\n",
    "    options=['Simple', 'Full with CPU', 'Accelerated GPU', 'Demo (Use prior data)'],\n",
    "    tooltips=['Ilab Simple Pipeline', 'Full Pipe running only on a CPU', 'Accelerated pipeline run on a GPU', 'Demo Run with previously created data'],\n",
    "    description='Processing', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "epoch=widgets.ToggleButtons(\n",
    "    options=['1', '2', '3', '4', '5', '10', '15'],\n",
    "    description='Epochs:',\n",
    "    disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "it=widgets.ToggleButtons(\n",
    "    options=['1', '3', '5','10','20','50','100','200'],\n",
    "    description='Iterations:',\n",
    "    disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "\n",
    "print(\"Select to Continue or to Train the model\")\n",
    "#pipe3.value=pipe.value\n",
    "display(pipe3)\n",
    "display(epoch)\n",
    "display(it)\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 5. Train the Model\"))\n",
    "        display(widgets.HTML(H2+\"5.1 Select the model training pipeline\"))\n",
    "        display(widgets.HTML(Norm+\"<br>Select the processsing pipeline for this run. Use Fast for the first run on new data to verify configuration.\"))\n",
    "        display(pipe3)\n",
    "        display(epoch)\n",
    "        display(it)\n",
    "        display(widgets.HTML(Norm+\"<br>\"))\n",
    "else:\n",
    "    print(\"After choosing your training options, please select and run the following cell\")\n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run the model training\n",
    "\n",
    "Model training takes 10 minutes for 1 epoch and 1 iteration. This minimal training could be used for testing the generation and training for a new set of data.\n",
    "\n",
    "To produce a higher quality model, more epochs and iterations are needed for refining the model. This will require a proportionally longer time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path=\"data/\"+ use_case+\"/ilab_generated/\"\n",
    "train_data=data_path+\"train_gen.jsonl\"\n",
    "model_path=\"models/instructlab/granite-7b-lab\"\n",
    "#model_path='/home/jovyan/.cache/instructlab/models/instructlab/granite-7b-lab'\n",
    "trained_model_path=\"data/\"+ use_case+\"/new_model/\"\n",
    "\n",
    "#'Simple (Fast)', 'Full with CPU', 'Accelerated GPU','DDemo (Use prior data)'\n",
    "if pipe3.value=='Demo (Use prior data)':\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(H1+\"Step 5. Train the Model\"))\n",
    "            display(widgets.HTML(H2+\"5.2 Run the model training\"))\n",
    "            display(widgets.HTML(Norm+\"Using previously trained data\"))\n",
    "    print(\"Using previously trained data\")\n",
    "else:\n",
    "    file_cnt=0\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename[:15]=='train_gen.jsonl': file_cnt+=1\n",
    "        elif filename[:14]=='test_gen.jsonl': file_cnt+=1\n",
    "    if file_cnt < 2 or os.path.getsize(directory+train_name) < 5 or os.path.getsize(directory+test_name) < 5:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\"))\n",
    "        print(\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\") \n",
    "    \n",
    "    if not os.path.exists(trained_model_path):\n",
    "        print(\"Create directory: \" + trained_model_path)\n",
    "        !mkdir {trained_model_path}\n",
    "    ep=int(epoch.value)\n",
    "    its=int(it.value)\n",
    "    if pipe3.value=='Simple':\n",
    "        if dual_screen:\n",
    "            with out:\n",
    "                display(widgets.HTML(H1+\"Step 5. Train the Model\"))\n",
    "                display(widgets.HTML(H2+\"5.2 Run model training\"))\n",
    "                display(widgets.HTML(Norm+\"Train with a GPU with simple pipeline\"))\n",
    "        print(\"Train with simple pipeline with a GPU\")\n",
    "        shell_command = f\"ilab model train --pipeline simple --model-path {model_path} --data-path {data_path} --device cpu --num-epochs {ep} --iters {its}\"\n",
    "    if pipe3.value=='Full with CPU':\n",
    "        if dual_screen:\n",
    "            with out:\n",
    "                display(widgets.HTML(H1+\"Step 5. Train the Model\"))\n",
    "                display(widgets.HTML(H2+\"5.2 Run model training\"))\n",
    "                display(widgets.HTML(Norm+\"Train with a CPU\"))\n",
    "        print(\"Train with a CPU\")\n",
    "        shell_command = f\"ilab model train --pipeline full --model-path {model_path} --data-path {train_data} --device cpu\"\n",
    "    elif pipe3.value=='Accelerated GPU':\n",
    "        if dual_screen:\n",
    "            with out:\n",
    "                display(widgets.HTML(H1+\"Step 5. Train the Model\"))\n",
    "                display(widgets.HTML(H2+\"5.2 Run model training\"))\n",
    "                display(widgets.HTML(Norm+\"Train with a GPU\"))\n",
    "        print(\"Train with a GPU\")\n",
    "        shell_command = f\"ilab model train --pipeline accelerated --device cuda --model-path {model_path} --data-path {train_data} --num-epochs {ep} --iters {its}\"\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"Running: !\"+shell_command))\n",
    "            !{shell_command}\n",
    "            if pipe3.value=='Accelerated GPU' or pipe3.value=='Accelerated GPU with 4b Quantization':\n",
    "                !ilab model evaluate --benchmark mmlu\n",
    "    if not dual_screen:\n",
    "        print(\"Running: !\"+shell_command)\n",
    "        !{shell_command}\n",
    "        if pipe3.value=='Accelerated GPU' or pipe3.value=='Accelerated GPU with 4b Quantization':\n",
    "            !ilab model evaluate --benchmark mmlu\n",
    "    #Move the model to the use_case/new_model directory\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"Move the trained model to the directory: \"+trained_model_path))\n",
    "    print(\"Moving the trained model to the directory: \"+trained_model_path)\n",
    "    !mv /home/jovyan/.local/share/instructlab/checkpoints/ggml-model-f16.gguf {trained_model_path}\n",
    "run.pause()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I2_train\"></a>\n",
    "# Step 6 Test the Model\n",
    "## 6.1 Run test on the model to see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ilab model test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n",
    "#shell_command=f\"ilab model evaluate --benchmark mmlu --model {ILAB_MODELS_DIR}/instructlab/granite-7b-test\"\n",
    "#!{shell_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the Inferencing with InstructLab notebook to assess improvements\n",
    "You have now completed InstructLab training. You can now select to run the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook to ask questions to both the base and InstructLab trained models and to compare answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H1+\"You have created a trained model for the \" + data_set.value + \" data set\"))\n",
    "        display(widgets.HTML(H2+\"To assess improvements, run the Inferencing with InstructLab notebook\"))\n",
    "else: \n",
    "    print(\"You have created a trained model for the \" + data_set.value + \" data set\")\n",
    "    print(\"To assess improvements, run the Inferencing with InstructLab notebook\")\n",
    "run.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"I1_conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated utilizing InstructLab for introducing datasets, data generation, model training, and model creation. This notebook produced an InstructLab trained model that can be used for inferencing. The following notebook [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) can be used to run queries on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I1_learn\"></a>\n",
    "# Learn More\n",
    "\n",
    "Proceed to run the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook run inferencing on the InstructLab trained model. This will allow you to interact with your model to see how well it performs on queries, both before it was trained and after InstrutLab training\n",
    "\n",
    "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
    "\n",
    "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
    "\n",
    "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© 2025 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:conda-instructlab-1.0.0]",
   "language": "python",
   "name": "conda-env-conda-instructlab-1.0.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
