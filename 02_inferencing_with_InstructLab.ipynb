{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Inferencing with InstructLab\n",
    "\n",
    "<ul>\n",
    "<li>Contributors: InstructLab team and IBM Research Technology Education team\n",
    "<li>Contact for questions and technical support: IBM.Research.JupyterLab@ibm.com\n",
    "<li>Provenance: IBM Research\n",
    "<li>Version: 1.0.8\n",
    "<li>Release date: 2024-08-30\n",
    "<li>Compute requirements: GPU: estimated 6 minutes\n",
    "<li>Memory requirements: 16 GB\n",
    "<li>Notebook set: InstructLab\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select Viewing Option \n",
    "\n",
    "This notebook was optimized for viewing the output in a separate panel:\n",
    "- If you would like to see the separate panel output, set `dual_screen` to *True* in the first line of the next cell and follow the steps below. \n",
    "- If you want the output inline with the notebook cells, set `dual_screen` to *False*. If you are running with the output inline with the notebook, please run the notebook cell by cell so that options can be selected.\n",
    "\n",
    "If you set `dual_screen` to *True*, perform the following:\n",
    "1. Right click on the same cell and select **Create New View for Output**.\n",
    "1. Drag the new **Output View** panel to the right side of the JupyterLab.\n",
    "1. Hide the File Browser by toggling the File Browser icon on the top left of the JupyterLab.\n",
    "1. To run the notebook, click on a notebook code cell, then from the top menu select *Kernel->Restart Kernel and Run All Cells*.\n",
    "1. Select options and the *Continue* button to progress with the notebook.\n",
    "\n",
    "**Note:** This notebook must be run with a GPU. If you are not running with a GPU, please select File->Hub Control Panel->Stop My Server, then Start My Server and the select GPU Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_screen = False\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import ipywidgets as widgets\n",
    "from ipynb_pause import flow\n",
    "\n",
    "H1 = \"<p style='font-family:IBM Plex Sans;font-size:28px'>\"\n",
    "H2 = \"<p style='font-family:IBM Plex Sans;font-size:24px'>\"\n",
    "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
    "Small = \"<p style='font-family:IBM Plex Sans;font-size:17px'>\"\n",
    "Ex = \"<p style='font-family:IBM Plex Sans;font-size:20px;font-style:italic'>\"\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "run=flow.display_mode(mode=dual_screen, output=out, color='darkblue')\n",
    "if dual_screen:\n",
    "    display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "This notebook is part of a sequential notebook set. Before using this notebook, please ensure that you have reviewed the first and second notebooks in the set:\n",
    "- [Configuring InstructLab](./00_configuring_InstructLab.ipynb)\n",
    "- [Training with InstructLab](./01_training_with_InstructLab.ipynb)\n",
    "\n",
    "The second notebook within this set showcases the generation of synthetic data utilizing InstructLab. It subsequently demonstrates how a large language model (LLM) can be effectively trained on this synthetic dataset. In current notebook, Both the pre-trained LLM and the LLM trained on the generated synthetic data are evaluated against a predefined set of questions to assess their respective performance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* <a href=\"#I2_1\">Step 1. Import Libraries </a>\n",
    "* <a href=\"#I2_2\">Step 2. Load the Base Model and the InstructLab Trained Model</a>\n",
    "* <a href=\"#I2_3\">Step 3. Define a Function to Perform Inference on Base and Trained Models </a>\n",
    "* <a href=\"#I2_4\">Step 4. Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance</a>\n",
    "* <a href=\"#I2_conclusion\">Conclusion</a>\n",
    "* <a href=\"#I2_learn\">Learn more</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I2_1\"></a>\n",
    "# Step 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 1. Import Libraries\"))\n",
    "        display(widgets.HTML(Norm+\"We import the required libraries to generate Q&A session with LLMs\"))\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "with open('instructlab.json', 'r') as f:\n",
    "    jsonState = json.load(f)\n",
    "\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(Norm+\"Imports completed\"))\n",
    "else: \n",
    "    print(f\"Imports completed\")\n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I2_2\"></a>\n",
    "# Step 2. Load the Base Model and the InstructLab Trained Model\n",
    "The data set that was used in that last run of the [Training with InstructLab](./01_training_with_InstructLab.ipynb) notebook is preselected for inferencing. You may select an alternative dataset if you wish.\n",
    "You can select the trained model to use for inferencing from:\n",
    "* **Fine Tuned Model** - A previously trained fine tuned model to demonstrate inferencing.\n",
    "* **Newly Trained Model** - Your trained model from prior runs of the [Training with InstructLab](./01_training_with_InstructLab.ipynb) notebook. You can optionally place a model you wish to make comparisons with in the */data/data_set/new_model* directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = widgets.ToggleButtons(\n",
    "    options=['2024 Oscars', 'Quantum', 'Agentic AI', 'Your Content 1', 'Your Content 2'],\n",
    "    tooltips=['2024 Oscar Awards Ceremony', 'Quantum Roadmap and Patterns', 'Artificial Intelligence Agents', 'Your own uploaded content dataset 1', 'Your own uploaded content dataset 2'],\n",
    "    description='Dataset:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "model = widgets.ToggleButtons(\n",
    "    options=['Fine Tuned Model', 'Newly Tuned Model'],\n",
    "    tooltips=['Tested fine tuned model', 'Newly tuned model'],\n",
    "    description='Model:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "\n",
    "print(\"\\nPlease select correct document which was used in notebook 01_training_with_InstructLab\")\n",
    "print(\"\\nSelect the content (Last used in training is preselected):\")\n",
    "data_set.value=jsonState[\"last_use_case\"]\n",
    "display(data_set)\n",
    "display(model)\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 2. Load the Base Model and the InstructLab Trained Model\"))\n",
    "        display(widgets.HTML(Norm+\"<br>Select the content (Last used in training is preselected):\"))\n",
    "        display(data_set)\n",
    "        display(model)\n",
    "        display(widgets.HTML(Small+\"Select 'Demo Content' or 'Your Own Content' if you provided your own created QNA file\"))\n",
    "else:\n",
    "    print(\"After choosing your dataset for inferencing, select the following cell and continue running the notebook\")         \n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H1+\"Step 2. Load the Base Model and the InstructLab Trained Model\"))\n",
    "        display(widgets.HTML(Norm+\"Using Data Set: \" + data_set.value))\n",
    "else:\n",
    "    print(\"Using Data Set: \" + data_set.value)\n",
    "if data_set.value=='2024 Oscars':\n",
    "    use_case=\"oscars\"\n",
    "elif data_set.value=='Quantum':\n",
    "    use_case=\"quantum\"\n",
    "elif data_set.value=='Agentic AI':\n",
    "    use_case=\"agentic_ai\"\n",
    "elif data_set.value=='Your Content 1':\n",
    "    use_case=\"your_content_1\"\n",
    "elif data_set.value=='Your Content 2':\n",
    "    use_case=\"your_content_2\"       \n",
    "else:\n",
    "    print(\"ERROR: Please select correct document which was used in notebook 01_training_with_InstructLab\")\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"ERROR: Please select correct document which was used in notebook 01_training_with_InstructLab\"))\n",
    "            \n",
    "if model.value == 'Fine Tuned Model':\n",
    "    directory=\"/fine_tuned_models/\"+use_case+\"/\"\n",
    "else:\n",
    "    directory=\"/data/\"+ use_case+\"/new_model/\"\n",
    "    \n",
    "notebook_dir=os.getcwd()\n",
    "os.chdir('/home/jovyan/')\n",
    "pwd= os.getcwd()\n",
    "\n",
    "base_model_path = notebook_dir +\"/models/granite-7b-lab-Q4_K_M.gguf\"\n",
    "trained_model_path = notebook_dir + directory+ \"ggml-model-f16.gguf\"\n",
    "\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(Norm+\"Base model directory: \"+base_model_path))\n",
    "        display(widgets.HTML(Norm+\"Trained model directory: \"+trained_model_path))\n",
    "        display(widgets.HTML(Norm+\"Both LLMs are loaded\"))\n",
    "print(\"Base model directory: \"+base_model_path)\n",
    "print(\"Trained model directory: \"+trained_model_path)\n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"I2_3\"></a>\n",
    "# Step 3. Define a Function to Perform Inference on Base and Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(base_model_path, trained_model_path):\n",
    "    _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "    Current conversation:\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "    \n",
    "    base_llm = LlamaCpp(model_path=base_model_path,\n",
    "                   verbose=False,\n",
    "                   n_gpu_layers=25,\n",
    "                   max_tokens=90,\n",
    "                   temperature=0,\n",
    "                   top_k=1\n",
    "                  )\n",
    "    trained_llm = LlamaCpp(model_path=trained_model_path,\n",
    "                   verbose=False,\n",
    "                   n_gpu_layers=25,\n",
    "                   max_tokens=90,\n",
    "                   temperature=0,\n",
    "                   top_k=1\n",
    "                  )   \n",
    "\n",
    "    PROMPT = PromptTemplate( input_variables=[\"input\"], \n",
    "                            template=_DEFAULT_TEMPLATE\n",
    "                            )\n",
    "    \n",
    "    chain1 = PROMPT | base_llm | StrOutputParser()\n",
    "    chain2 = PROMPT | trained_llm | StrOutputParser()\n",
    "    if dual_screen:\n",
    "        with out:\n",
    "            display(widgets.HTML(Norm+\"Ready to ask questions in the code window\"))\n",
    "   \n",
    "    while True:\n",
    "        question = input(\"Ask me a question (type 'exit' to end): \")        \n",
    "        if question.lower() == 'exit':\n",
    "            if dual_screen:\n",
    "                with out:\n",
    "                    display(widgets.HTML(Norm+\"Exiting this Q&A session.\"))\n",
    "            else:\n",
    "                print(\"Exiting this Q&A session.\")\n",
    "            break\n",
    "        else:            \n",
    "            if dual_screen:\n",
    "                with out:\n",
    "                    display(widgets.HTML(Norm+\"<br>You asked: \" + question))\n",
    "                    answer1 = chain1.invoke(question)\n",
    "                    answer1= answer1.split('Human',1)[0]\n",
    "                    display(widgets.HTML(Norm+\"Base Model Answer: \" + answer1))\n",
    "                    answer2 = chain2.invoke(question)\n",
    "                    answer2= answer2.split('Human',1)[0] \n",
    "                    display(widgets.HTML(Norm+\"Trained Model Answer: \" + answer2))\n",
    "            else:\n",
    "                print(\"You asked: \", question)\n",
    "                answer1 = chain1.invoke(question)\n",
    "                answer1= answer1.split('Human',1)[0]\n",
    "                print (\"Base Model Answer: \",answer1)\n",
    "                answer2 = chain2.invoke(question)\n",
    "                answer2= answer2.split('Human',1)[0] \n",
    "                print (\"Trained Model Answer: \",answer2)\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(widgets.HTML(H1+\"Step 3. Define a Function to Perform Inference on Base and Trained Models\"))\n",
    "        display(widgets.HTML(Norm+\"Function defined\"))\n",
    "else:\n",
    "    print(f\"Function to perform inference on LLMs defined\")\n",
    "    \n",
    "run.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"I2_4\"></a>\n",
    "# Step 4. Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Sample questions that can be asked to LLM\n",
    "\n",
    "The following are sample questions derived from the data used to generate synthetic data, which was then employed to train the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Sample Questions\n",
    "with open(notebook_dir+'/data/' + use_case + '/questions.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        display(widgets.HTML(Norm+line))\n",
    "\n",
    "if dual_screen:\n",
    "    with out:\n",
    "        display(widgets.HTML(H1+\"Step 4. Run Interactive Q&A Session\"))\n",
    "        display(widgets.HTML(Norm+\"Processing may take a couple minutes on the first run...\"))\n",
    "        model_inference(base_model_path, trained_model_path)\n",
    "else:\n",
    "    print(\"Processing may take several minutes on the first run...\")\n",
    "    model_inference(base_model_path, trained_model_path)\n",
    "run.resume()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"I2_conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated inferencing with models produced using InstructLab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I2_learn\"></a>\n",
    "# Learn More\n",
    "\n",
    "Proceed to run the [Training with Red Hat AI InstructLab Service](./03_training_with_RH_AI_InstructLab_Service.ipynb) notebook utilize the Red Hat AI InstructLab IBM Cloud-based service. \n",
    "\n",
    "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
    "\n",
    "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
    "\n",
    "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2025 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:conda-instructlab-1.0.0]",
   "language": "python",
   "name": "conda-env-conda-instructlab-1.0.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
