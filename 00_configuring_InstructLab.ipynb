{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7ih7e5O6rX_",
    "tags": []
   },
   "source": [
    "# Configuring InstructLab\n",
    "\n",
    "<ul>\n",
    "<li>Contributors: InstructLab team and IBM Research Technology Education team\n",
    "<li>Contact for questions and technical support: IBM.Research.JupyterLab@ibm.com\n",
    "<li>Provenance: IBM Research\n",
    "<li>Version: 1.0.9\n",
    "<li>Release date: 2024-11-14\n",
    "<li>Compute requirements: GPU: estimated 4 to 15 minutes (up to 4 min for cell 6, 8 min for cell 7. and 5 min for cell 8)\n",
    "<li>Memory requirements: 16 GB\n",
    "<li>Notebook set: InstructLab\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ics9GgZ-6rYB",
    "tags": []
   },
   "source": [
    "# Summary\n",
    "This notebook set demonstrates InstructLab, an open source AI project that facilitates knowledge and skills contributions to Large Language Models (LLMs). InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081). The open source InstructLab repository is available [here](https://github.com/instructlab/instructlab) and provides additional documentation on using InstructLab.\n",
    "\n",
    "InstructLab  can be instantiated in several different forms, depending on the processing capabilities available. InstructLab can take the form of an open source installation or a Red Hat AI InstructLab installation. The open source installation can be run on a range of hardware from a laptop to a build your own (BYO) server instance running on a Virtual Machine (VM). The below figure shows the different available instantiations of InstructLab.\n",
    "\n",
    "<img src=\"./data/images/experiences.png\" width=\"800\">\n",
    "\n",
    "In this notebook set, we will be demonstrating both the open source version running on a VM server and Red Hat Enterprise Linux AI InstructLab running on an IBM Cloud Server.\n",
    "\n",
    "The open source version running on a server is demonstrated in the following notebooks that are run sequentially:\n",
    "- [Configuring InstructLab](./00_configuring_InstructLab.ipynb)\n",
    "- [Training with InstructLab](./01_training_with_InstructLab.ipynb)\n",
    "- [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb)\n",
    "\n",
    "The Red Hat Enterprise Linux AI InstructLab is demonstrated running as a service in the IBM Cloud by running the following notebooks:\n",
    "- [Configuring InstructLab](./00_configuring_InstructLab.ipynb)\n",
    "- [Training with Red Hat AI InstructLab Service](./03_training_with_RH_AI_InstructLab_Service.ipynb)\n",
    "- [Inferencing with Redhat-AI-InstructLab Trained Model](./04_inferencing_with_RH_AI_InstructLab_Service.ipynb)\n",
    "**Note:** The **Configuring InstructLab** notebook is run before the other Red Hat AI InstructLab notebooks to ensure that the *granite 7b model* is installed for inference comparisons as the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnQ88KKN6rYC",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Configuring InstructLab\n",
    "\n",
    "This notebook demonstrates the configuration of InstructLab. The InstructLab method consists of three major components:\n",
    "* **Taxonomy-driven data curation:**  The taxonomy is a set of training data curated by humans as examples of new knowledge and skills for the model.\n",
    "* **Large-scale synthetic data generation:** A teacher model is used to generate new examples based on the seed training data. Recognizing that synthetic data can vary in quality, the InstructLab method adds an automated step to refine the example answers, ensuring they are grounded and safe.\n",
    "* **Iterative model alignment tuning:** The model is retrained based on the synthetic data. The InstructLab method includes two tuning phases: knowledge tuning, followed by skill tuning.\n",
    "\n",
    "In this notebook, we will demonstrate the following:\n",
    "1. Checking the InstructLab installation\n",
    "2, Configuring InstructLab for use\n",
    "3. Installing InstructLab LLM models\n",
    "\n",
    "**Note:** This notebook must be run within a GPU session. If you are not running with a GPU, please select **File->Hub Control Panel->Stop My Server**, then **Start My Server** and then select a GPU Session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNwwcY4l6rYC",
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "* <a href=\"#I0_preconfig\">Step 0. Environment Preconfiguration</a>\n",
    "* <a href=\"#I0_init\">Step 1. Check the Starting Configuration</a>\n",
    "* <a href=\"#I0_config\">Step 2. Configure InstructLab</a>\n",
    "* <a href=\"#I0_down\">Step 3. Download Models</a>\n",
    "* <a href=\"#I0_conclusion\">Conclusion</a>\n",
    "* <a href=\"#I0_learn\">Learn more</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlk9FjFB6rYD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"#I0_preconfig\"></a>\n",
    "# Step 0. Environment Preconfiguration\n",
    "This step has already been performed for this JupyterLab environment and requires no work by the user. This information is provided in case the user is setting up their own environment on their own laptop or server.\n",
    "\n",
    "The full steps for a direct installation are [here](https://github.com/instructlab/instructlab).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTGUHWBTA44y",
    "outputId": "8a83b02e-d825-42bc-e0ff-172167b9fc61"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE8DwgfEBhF3"
   },
   "outputs": [],
   "source": [
    "command = f\"\"\"\n",
    "pip uninstall instructlab<<EOF\n",
    "Y\n",
    "\"\"\"\n",
    "\n",
    "# Using the ! operator to run the command\n",
    "#!echo \"Running ilab config init\"\n",
    "#!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN61V-x7V7rl"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKJKDLYnV8Gf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0WsHUZ3BVD9",
    "outputId": "a247c585-7cb0-4dac-d294-89d9e87a7b49"
   },
   "outputs": [],
   "source": [
    "!pip install instructlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5EcJoSS6rYD",
    "tags": []
   },
   "source": [
    "<a id=\"#I0_check\"></a>\n",
    "# Step 1. Check the Starting Configuration\n",
    "\n",
    "## 1.1 Check for a GPU\n",
    "\n",
    "This code cell checks for a GPU in the configuration as is required to run the kernel.\n",
    "\n",
    "If you receive an error about a GPU not in the configuration, preform the following:\n",
    "1. Select File->Hub Control Panel.\n",
    "1. On the Hub Control Panel, select the blue \"Stop My Server\" button.\n",
    "1. Then select \"Start My Server\" and choose Session with a GPU.\n",
    "1. Check at the upper right that the kernel being run is \"conda-instructlab-latest\". If not, select the kernel and switch to the correct kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLDdSBiX6rYD",
    "outputId": "eb3ee0c9-327c-4ef4-f3e8-4b57d8fbad48"
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# torch and cuda version check\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "if torch.cuda.is_available() is False:\n",
    "    print(\"No GPU in configuration\")\n",
    "else:\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(\"GPU is Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_TTU5vUhXvM",
    "tags": []
   },
   "source": [
    "## 1.2 Check InstructLab Version and GPU Offload\n",
    "\n",
    "Check that InstructLab version 0.23.1 is installed properly and is configured for using a GPU.\n",
    "\n",
    "The first line from 'InstructLab' section should read\n",
    "```\n",
    "instructlab.version: 0.23.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqjqIGE06rYE",
    "outputId": "d3af3774-8321-43c7-8c22-441e8ef40e04"
   },
   "outputs": [],
   "source": [
    "!ilab system info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv5uMvQ-a-ZF",
    "tags": []
   },
   "source": [
    "<a id=\"#I0_config\"></a>\n",
    "# Step 2. Configure InstructLab\n",
    "\n",
    "## 2.1 Create InstructLab config file\n",
    "The InstructLab configuration is captured in the *config.yaml* file. This step creates the config.yaml file and sets:\n",
    "- **taxomony_path = taxonomy** - the root location of the taxonomy is set to the taxonomy folder in instructlab-latest\n",
    "- **model_path = models/merlinite-7b-lab-Q4_K_M.gguf** - the default model is set to merlinite\n",
    "\n",
    "**Note:** The default directories for InstructLab are the following. If you initialize InstructLab on your own system, it will default to the following:\n",
    "* **Downloaded Models:**  ~/.cache/instructlab/models/ - Contains all downloaded large language models, including the saved output of ones you generate with ilab.\n",
    "* **Synthetic Data:** ~/.local/share/instructlab/datasets/ - Contains data output from the SDG phase, built on modifications to the taxonomy repository.\n",
    "* **Taxonomy:** ~/.local/share/instructlab/taxonomy/ - Contains the skill and knowledge data.\n",
    "* **Training Output:** ~/.local/share/instructlab/checkpoints/ - Contains the output of the training process.\n",
    "* **config.yaml:** ~/.config/instructlab/config.yaml - Contains the config.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QX9s4XZx6rYF",
    "outputId": "44058e98-56d8-4237-8b9b-fd82fc7d6ab6"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "base_dir=\"/root/\"\n",
    "model_dir=\"models\"\n",
    "#Choose the base model as granite or mixtral\n",
    "# Choose as quantized granite model\n",
    "model_name=\"granite-7b-lab-Q4_K_M.gguf\"\n",
    "\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "taxonomy_path='taxonomy'\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"config.yaml\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "    print(f\"ilab was already initialized. {file_name} has been deleted. Reinitialized\")\n",
    "else:\n",
    "    print(f\"ilab was not initialized yet. {file_name} does not exist.\")\n",
    "\n",
    "#Remove old data\n",
    "if os.path.exists(\"taxonomy\"):\n",
    "    print(\"removing taxonomy\")\n",
    "    shutil.rmtree(\"taxonomy\")\n",
    "if os.path.exists(base_dir+\".cache/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".cache/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".cache/instructlab\")\n",
    "if os.path.exists(base_dir+\".config/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".config/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".config/instructlab\")\n",
    "if os.path.exists(base_dir+\".local/share/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".local/share/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".local/share/instructlab\")\n",
    "\n",
    "print(f\"ilab model is {model_path}.\")\n",
    "print('#############################################################')\n",
    "print(' ')\n",
    "\n",
    "command = f\"\"\"\n",
    "ilab config init<<EOF\n",
    "{taxonomy_path}\n",
    "Y\n",
    "{model_path}\n",
    "4\n",
    "1\n",
    "EOF\n",
    "\"\"\"\n",
    "\n",
    "# Using the ! operator to run the command\n",
    "!echo \"Running ilab config init\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8rbeRKE6rYF"
   },
   "source": [
    "## 2.2 Display the config.yaml file\n",
    "We examine the base configuration for identifying parameters for changing in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdZuLgLmcEO5",
    "outputId": "99dd4975-a436-43ec-8c12-adbecaf96e0a"
   },
   "outputs": [],
   "source": [
    "!ilab config show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZflN-eeu6rYF",
    "outputId": "400436c3-e137-4576-e4f6-2109be90e931",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to copy config.yaml to local directory\n",
    "!cp /root/.config/instructlab/config.yaml .\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAL743VA6rYH"
   },
   "source": [
    "## 2.3 Customize LLM Models and copy to notebook for use\n",
    "\n",
    "This cell changes the models to use for the generate stage. The mistral model as the teacher model in the generate step and as the student model to be trained.\n",
    "\n",
    "If you want to customize other models for generation or the training phase, you would specify the models in this step.\n",
    "\n",
    "This step specifies that the models to be used will be from this notebook's models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QILAQZYY6rYH",
    "outputId": "25611b55-6f1b-4d30-e285-30fb922e9fc8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use ruamel.yaml to load the yaml file to preserve comments\n",
    "import ruamel.yaml\n",
    "yaml = ruamel.yaml.YAML()\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.load(file)\n",
    "\n",
    "#Upate to use the same models and just change the directory\n",
    "teacher_model_path = \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "base_model_path = \"models/instructlab/granite-7b-lab\"\n",
    "#judge_model_path = \"models/prometheus-eval/prometheus-8x7b-v2.0\"\n",
    "\n",
    "#config['evaluate']['mt_bench']['judge_model'] = judge_model_path\n",
    "#config['evaluate']['mt_bench_branch']['judge_model'] = judge_model_path\n",
    "config['generate']['model'] = teacher_model_path\n",
    "config['generate']['teacher']['model_path']= teacher_model_path\n",
    "#config['train']['phased_mt_bench_judge']=judge_model_path\n",
    "\n",
    "# Save the updated config.yaml file\n",
    "yaml.default_flow_style=False\n",
    "with open('config.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "\n",
    "#copy the config file to the .config/instructlab/ where it is used by InstructLab\n",
    "!cp config.yaml {base_dir}.config/instructlab/\n",
    "\n",
    "print(\"Updated config.yaml successfully.\\n\")\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dZgO-FZ6rYH"
   },
   "source": [
    "<a id=\"#I0_down\"></a>\n",
    "# Step 3. Download Models\n",
    "The models that will be used in the InstructLab processing are downloaded in this step. Additional steps can be added if other models are used in processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI-25zVd6rYH"
   },
   "source": [
    "## 3.1. Download the merlinite and mistral-7b-instruct-v0.2.Q4_K_M models\n",
    "\n",
    "The merlinite model will be used as the teacher model for the simple pipeline in the [Training with InstructLab](./01_training_with_InstructLab.ipynb) notebook.\n",
    "\n",
    "The mistral-7b-instruct-v0.2.Q4_K_M model will be used as the teacher model for the full pipeline in the same notebook.\n",
    "\n",
    "The granite07b-lab.gguf model is a quantized version oft eh granite-7b-lab model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCa2va8r6rYH",
    "outputId": "4377980d-e4b4-47f7-b49f-26dcbfcdca7b"
   },
   "outputs": [],
   "source": [
    "models_dir=\"models\"\n",
    "hf_token = \"\"\n",
    "!ilab model download --hf-token {hf_token} --model-dir {models_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k55QNXBDwb1O",
    "outputId": "9cc9e5ed-fc89-4bb2-cad3-427cdac15d24"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d84cf2D6rYH",
    "tags": []
   },
   "source": [
    "## 3.2. Optionally Download the granite 7b safe tensors model\n",
    "\n",
    "Download the *granite-7b-lab* model. The  *granite-7b-lab* model is used as:\n",
    "* The default base model for training in the [Training with InstructLab](./01_training_with_InstructLab.ipynb) notebook.\n",
    "* The default base model for inferencing comparisons in the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook.\n",
    "* The base model for inferencing comparisons in the [Inferencing with Redhat-AI-InstructLab Trained Model](./04_inferencing_with_RH_AI_InstructLab_Service.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcYf96yxyr_X",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!ilab model download --repository instructlab/granite-7b-lab --hf-token {hf_token} --model-dir {models_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg-mhvIb6rYI",
    "tags": []
   },
   "source": [
    "## 3.3. Optionally download the prometheus-8x7b-v2.0 model\n",
    "The *prometheus-8x7b-v2.0* model is used as a judege model for multi-phase training and benchmark evaluation. This model is not required for simple or full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hT6ZHGmx6rYI"
   },
   "outputs": [],
   "source": [
    "#!ilab model download --repository prometheus-eval/prometheus-8x7b-v2.0 --hf-token {hf_token} --model-dir {models_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byxo95Jl6rYI",
    "tags": []
   },
   "source": [
    "<a id=\"I0_conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated setting up the InstructLab environment to be ready for introducing datasets for data generation, training and model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGPhEVzS6rYI"
   },
   "source": [
    "<a id=\"I0_learn\"></a>\n",
    "# Learn More\n",
    "\n",
    "Proceed to run the [Training with InstructLab](./01_training_with_InstructLab.ipynb) notebook to introduce datasets, perform synthetic data generation and train InstructLab models.\n",
    "\n",
    "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
    "\n",
    "This notebook is based on the open source InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
    "\n",
    "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp0Hqmgb6rYI"
   },
   "source": [
    "Â© 2025 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
